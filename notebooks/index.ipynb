{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Sentiments to Strategies: Building an NLP Model for Brand Engagement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "from IPython.display import Image\n",
    "`\n",
    "Image(url = \"https://storage.ning.com/topology/rest/1.0/file/get/3780584426?profile=original\",width = 1000, height=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was sourced from data.world provided by CrowdFlower which has tweets about Apple and Google from the South by Southwest (SXSW) conference. The tweet labels were crowdsourced and reflect which emotion they convey and what product/service/company this emotion is directed at based on the content.\n",
    "\n",
    "There are 9093 records and 3 features in this data.\n",
    "\n",
    "Associated columns in the dataset are:\n",
    "- `tweet_text`: Contains the text of the tweets.\n",
    "\n",
    "- `emotion_in_tweet_is_directed_at`: Indicates the brand or product mentioned in the tweet (many missing values).\n",
    "\n",
    "- `is_there_an_emotion_directed_at_a_brand_or_product`: Categorizes the sentiment as \"Positive emotion,\" \"Negative emotion,\" or potentially other classes.\n",
    "\n",
    "The column names will be renamed to manageable ones in the data cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Preprocessing and Feature Extraction\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Handling Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure necessary NLTK resources are downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataOverview():\n",
    "    \"\"\"\n",
    "    This class takes a dataframe and returns basic information.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def read_head(self):\n",
    "        \"\"\"Returns the first 5 rows\"\"\"\n",
    "        return self.data.head()\n",
    "\n",
    "    def read_columns(self):\n",
    "        \"\"\"Returns the columns of the DataFrame\"\"\"\n",
    "        return self.data.columns\n",
    "\n",
    "    def read_info(self):\n",
    "        \"\"\"Returns the features, datatypes and non-null count\"\"\"\n",
    "        return self.data.info()\n",
    "    def read_describe(self):\n",
    "        \"\"\"Returns the statistical summary of the dataset\"\"\"\n",
    "        return self.data.describe()\n",
    "    def read_shape(self):\n",
    "        \"\"\"Returns the number of rows and columns\"\"\"\n",
    "        return self.data.shape\n",
    "    def read_unique(self,column_name):\n",
    "        \"\"\"Returns unique values for a specific column\"\"\"\n",
    "        if column_name in self.data.columns:\n",
    "            return self.data[column_name].unique()\n",
    "        else:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    def read_corr(self):\n",
    "        \"\"\"Returns a correlation dataframe\"\"\"\n",
    "        return self.data.corr()\n",
    "\n",
    "    def read_corr_wrt_target(self, target='churn'):\n",
    "        \"\"\"Returns a Series containing the correlation of features with respect to target\"\"\"\n",
    "        return self.data.corr()[target].sort_values(ascending=False)\n",
    "\n",
    "    def read_multicollinearity(self, target='churn'):\n",
    "        \"\"\"Returns a correlation dataframe without the target\"\"\"\n",
    "        return self.data.corr().iloc[0:-1, 0:-1]\n",
    "\n",
    "    def read_na(self):\n",
    "        \"\"\"Returns the sum of all null values per feature\"\"\"\n",
    "        return self.data.isna().sum()\n",
    "\n",
    "    def read_duplicated(self):\n",
    "        \"\"\"Returns the sum of all duplicated records\"\"\"\n",
    "        return self.data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data\n",
    "filepath='../data/tweet_product_company.csv'\n",
    "df = pd.read_csv(filepath,encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate datapreparation object\n",
    "dprep = DataOverview(data=df)\n",
    "\n",
    "# First 5 lines of the DataFrame\n",
    "dprep.read_head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming the column names for easier work and readability as they are very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for ease of work\n",
    "df.rename(columns = {'tweet_text': 'Text', 'emotion_in_tweet_is_directed_at': 'Product/Brand', \n",
    "                     'is_there_an_emotion_directed_at_a_brand_or_product':'Emotion'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring features and their datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Text           9092 non-null   object\n",
      " 1   Product/Brand  3291 non-null   object\n",
      " 2   Emotion        9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dprep.read_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text                1\n",
       "Product/Brand    5802\n",
       "Emotion             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring feature null values\n",
    "dprep.read_na()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product has 5802 nulls values i.e 64% of all records in the dataset, while Text has 1 missing records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the Product/Brand and Emotion columns to check their unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iPhone', 'iPad or iPhone App', 'iPad', 'Google', nan, 'Android',\n",
       "       'Apple', 'Android App', 'Other Google product or service',\n",
       "       'Other Apple product or service'], dtype=object)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprep.read_unique('Product/Brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprep.read_unique('Emotion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Product/Brand: We observe a lof of information on different products and services for the different brands.There is also nan values.\n",
    "- Emotion: We observe 'I can't tell' emotion which is ambigous, and thus will need handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we were missing the body of text for 1 tweet and a total of 5,802 tags for which product/brand the corresponding tweet was about. Let's start with looking at the missing tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy for cleaning purposes\n",
    "df_1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Product/Brand</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Text Product/Brand                             Emotion\n",
       "6  NaN           NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[df_1['Text'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuable information(customer sentiment) is missing from this Text, thus worthless, we can drop the record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Text           9092 non-null   object\n",
      " 1   Product/Brand  3291 non-null   object\n",
      " 2   Emotion        9092 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 284.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_1 = df_1[df_1['Text'].notna()]\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at Product/Brand missing values which has a total of 5801 missing tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Product/Brand</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>SPIN Play - a new concept in music discovery for your iPad from @mention &amp;amp; spin.com {link} #iTunes #sxsw @mention</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp;amp; Android: Whether youÛªre getting friend... {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and weÛªll make you one!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            Text  \\\n",
       "5   @teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd   \n",
       "16                                                  Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw   \n",
       "32                                           Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}   \n",
       "33                                                                                                 Anyone at  #sxsw want to sell their old iPad?   \n",
       "34                                                                 Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?   \n",
       "35                                  At #sxsw.  Oooh. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link}   \n",
       "37                         SPIN Play - a new concept in music discovery for your iPad from @mention &amp; spin.com {link} #iTunes #sxsw @mention   \n",
       "39                                                                        VatorNews - Google And Apple Force Print Media to Evolve? {link} #sxsw   \n",
       "41              HootSuite - HootSuite Mobile for #SXSW ~ Updates for iPhone, BlackBerry &amp; Android: Whether youÛªre getting friend... {link}   \n",
       "42             Hey #SXSW - How long do you think it takes us to make an iPhone case? answer @mention using #zazzlesxsw and weÛªll make you one!   \n",
       "\n",
       "   Product/Brand                             Emotion  \n",
       "5            NaN  No emotion toward brand or product  \n",
       "16           NaN  No emotion toward brand or product  \n",
       "32           NaN  No emotion toward brand or product  \n",
       "33           NaN  No emotion toward brand or product  \n",
       "34           NaN  No emotion toward brand or product  \n",
       "35           NaN  No emotion toward brand or product  \n",
       "37           NaN  No emotion toward brand or product  \n",
       "39           NaN  No emotion toward brand or product  \n",
       "41           NaN  No emotion toward brand or product  \n",
       "42           NaN  No emotion toward brand or product  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[df_1['Product/Brand'].isna()].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation is that the tweets are not directed to any specific brand or product, we will thus use 'Unknown' as a placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Product/Brand'].fillna('Unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text             0\n",
       "Product/Brand    0\n",
       "Emotion          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the null values are handled\n",
    "df_1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Emotion Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5388\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy interpretability the emotion values can be cleaned to shorter formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['Emotion'] = df_1['Emotion'].replace({\n",
    "    'Positive emotion': 'Positive',\n",
    "    'Negative emotion': 'Negative',\n",
    "    'No emotion toward brand or product': 'Neutral',\n",
    "    \"I can't tell\": 'Unknown'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     5388\n",
       "Positive    2978\n",
       "Negative     570\n",
       "Unknown      156\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying emotion values\n",
    "df_1['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the tweets with 'Unknown' emotion values to check wether we can identify any patterns or we can easily tell/categorise them in the Neutral,Positive or Negative categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Product/Brand</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ÛÏ@mention &amp;quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Just what America needs. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>The queue at the Apple Store in Austin is FOUR blocks long. Crazy stuff! #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Hope it's better than wave RT @mention Buzz is: Google's previewing a social networking platform at #SXSW: {link}</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>It's funny watching a room full of people hold their iPad in the air to take a photo. Like a room full of tablets staring you down. #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9032</th>\n",
       "      <td>@mention yeah, we have @mention , Google has nothing on us :) #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>@mention Yes, the Google presentation was not exactly what I was expecting. #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9058</th>\n",
       "      <td>&amp;quot;Do you know what Apple is really good at? Making you feel bad about your Xmas present!&amp;quot; - Seth Meyers on iPad2 #sxsw #doyoureallyneedthat?</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>How much you want to bet Apple is disproportionately stocking the #SXSW pop-up store with iPad 2? The influencer/hipsters thank you</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       Text  \\\n",
       "90                                     Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh   \n",
       "102                         ÛÏ@mention &quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow   \n",
       "237                              Just what America needs. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "341                                                                          The queue at the Apple Store in Austin is FOUR blocks long. Crazy stuff! #sxsw   \n",
       "368                                       Hope it's better than wave RT @mention Buzz is: Google's previewing a social networking platform at #SXSW: {link}   \n",
       "...                                                                                                                                                     ...   \n",
       "9020              It's funny watching a room full of people hold their iPad in the air to take a photo. Like a room full of tablets staring you down. #SXSW   \n",
       "9032                                                                                    @mention yeah, we have @mention , Google has nothing on us :) #SXSW   \n",
       "9037                                                                      @mention Yes, the Google presentation was not exactly what I was expecting. #sxsw   \n",
       "9058  &quot;Do you know what Apple is really good at? Making you feel bad about your Xmas present!&quot; - Seth Meyers on iPad2 #sxsw #doyoureallyneedthat?   \n",
       "9066                    How much you want to bet Apple is disproportionately stocking the #SXSW pop-up store with iPad 2? The influencer/hipsters thank you   \n",
       "\n",
       "     Product/Brand  Emotion  \n",
       "90         Unknown  Unknown  \n",
       "102        Unknown  Unknown  \n",
       "237        Unknown  Unknown  \n",
       "341        Unknown  Unknown  \n",
       "368        Unknown  Unknown  \n",
       "...            ...      ...  \n",
       "9020       Unknown  Unknown  \n",
       "9032       Unknown  Unknown  \n",
       "9037       Unknown  Unknown  \n",
       "9058       Unknown  Unknown  \n",
       "9066         Apple  Unknown  \n",
       "\n",
       "[156 rows x 3 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "df_1[df_1['Emotion']=='Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observation, this tweets are difficult to classify without further context. Some might be genuine and some sarcastic and thus may not be useful for our models as we need to have labels. The solution is to drop them, as they also are only \n",
    "1.7% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     5388\n",
       "Positive    2978\n",
       "Negative     570\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the 'Unknown' in the Emotion column\n",
    "df_1 = df_1.loc[df_1['Emotion'] != 'Unknown']\n",
    "# Verifying they are dropped\n",
    "df_1['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've addressed null values, and cleaned up our dataset a little bit more, next step is to check for duplicates (check tweets repeated multiple times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 22 tweets that are duplicated. Lets have a closer look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Product/Brand</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Before It Even Begins, Apple Wins #SXSW {link}</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>Really enjoying the changes in Gowalla 3.0 for Android! Looking forward to seeing what else they &amp;amp; Foursquare have up their sleeves at #SXSW</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>Oh. My. God. The #SXSW app for iPad is pure, unadulterated awesome. It's easier to browse events on iPad than on the website!!!</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>RT @mention ÷¼ GO BEYOND BORDERS! ÷_ {link} ã_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5341</th>\n",
       "      <td>RT @mention ÷¼ Happy Woman's Day! Make love, not fuss! ÷_ {link} ã_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxsw</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxsw</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #SXSW</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>RT @mention RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8483</th>\n",
       "      <td>I just noticed DST is coming this weekend. How many iPhone users will be an hour late at SXSW come Sunday morning? #SXSW #iPhone</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8747</th>\n",
       "      <td>Need to buy an iPad2 while I'm in Austin at #sxsw. Not sure if I'll need to Q up at an Austin Apple store?</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  Text  \\\n",
       "468                                                                                                     Before It Even Begins, Apple Wins #SXSW {link}   \n",
       "776                                                              Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "2232                                                Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw   \n",
       "2559                                                          Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear   \n",
       "3950  Really enjoying the changes in Gowalla 3.0 for Android! Looking forward to seeing what else they &amp; Foursquare have up their sleeves at #SXSW   \n",
       "3962        #SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan   \n",
       "4897                   Oh. My. God. The #SXSW app for iPad is pure, unadulterated awesome. It's easier to browse events on iPad than on the website!!!   \n",
       "5338                                          RT @mention ÷¼ GO BEYOND BORDERS! ÷_ {link} ã_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter   \n",
       "5341                     RT @mention ÷¼ Happy Woman's Day! Make love, not fuss! ÷_ {link} ã_ #edchat #musedchat #sxsw #sxswi #classical #newTwitter   \n",
       "5881                                                 RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "5882                                                 RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #SXSW   \n",
       "5883                                                 RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "5884                                                 RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #SXSW   \n",
       "5885                                                 RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "6296                                    RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw   \n",
       "6297                                    RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #SXSW   \n",
       "6298                                    RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw   \n",
       "6299                                    RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #SXSW   \n",
       "6300                                    RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw   \n",
       "6546                                     RT @mention RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw   \n",
       "8483                  I just noticed DST is coming this weekend. How many iPhone users will be an hour late at SXSW come Sunday morning? #SXSW #iPhone   \n",
       "8747                                        Need to buy an iPad2 while I'm in Austin at #sxsw. Not sure if I'll need to Q up at an Austin Apple store?   \n",
       "\n",
       "           Product/Brand   Emotion  \n",
       "468                Apple  Positive  \n",
       "776              Unknown   Neutral  \n",
       "2232             Unknown   Neutral  \n",
       "2559               Apple  Positive  \n",
       "3950         Android App  Positive  \n",
       "3962             Android  Positive  \n",
       "4897  iPad or iPhone App  Positive  \n",
       "5338             Unknown   Neutral  \n",
       "5341             Unknown   Neutral  \n",
       "5881             Unknown   Neutral  \n",
       "5882             Unknown   Neutral  \n",
       "5883             Unknown   Neutral  \n",
       "5884             Unknown   Neutral  \n",
       "5885             Unknown   Neutral  \n",
       "6296              Google  Positive  \n",
       "6297             Unknown   Neutral  \n",
       "6298              Google  Positive  \n",
       "6299             Unknown   Neutral  \n",
       "6300             Unknown   Neutral  \n",
       "6546             Unknown   Neutral  \n",
       "8483              iPhone  Negative  \n",
       "8747                iPad  Positive  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1[df_1.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8914 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Text           8914 non-null   object\n",
      " 1   Product/Brand  8914 non-null   object\n",
      " 2   Emotion        8914 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 278.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_1.drop_duplicates(keep='first', inplace=True)\n",
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we move on to EDA as we have already done the data cleaning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to our business problem , we will isolate and analyze positive and negative tweets as a whole as well as on a company and product basis. However, before conducting any EDA on sentiments we need to tokenize,lematize and remove stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a function to perform tokenization, lemmatization and stop words. For tokenizer we will apply tweet tokenizer, because we are dealing with social media data, as it can handle special social media tokens such as @ , #, emojis etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Lemmatization and Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword list with additional punctuation\n",
    "stop_lst = stopwords.words('english') + list(string.punctuation)\n",
    "stop_lst += ['“', '”', '...', \"''\", '’', '``', '']\n",
    "\n",
    "def tokenize_lemmatize_remove_stopwords(corpus, preserve_case=False, strip_handles=True, \n",
    "                                        stop_list=None, additional_stopwords=None):\n",
    "    \"\"\"\n",
    "    Tokenizes, lemmatizes, and removes stopwords from a given corpus.\n",
    "    \n",
    "    Args:\n",
    "    - corpus: List of text to process.\n",
    "    - preserve_case: Whether to keep upper case letters as upper case.\n",
    "    - strip_handles: Whether to remove Twitter handles.\n",
    "    - stop_list: Base list of stopwords (default to NLTK stop words + punctuation).\n",
    "    - additional_stopwords: Optional additional words to exclude from the tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - List of clean lemmatized tokens without stopwords.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer and lemmatizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=preserve_case, strip_handles=strip_handles)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Use the provided stop list or default to stop_lst\n",
    "    if stop_list is None:\n",
    "        stop_list = stop_lst\n",
    "\n",
    "    # Extend stop list with additional custom stopwords\n",
    "    if additional_stopwords:\n",
    "        stop_list = stop_list + additional_stopwords\n",
    "\n",
    "    # Tokenize corpus\n",
    "    tokens = tokenizer.tokenize(','.join(corpus))\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens_cleaned = [lemmatizer.lemmatize(token).encode('ascii', 'ignore').decode() \n",
    "                       for token in tokens \n",
    "                       if token not in stop_list and not token.startswith('http')]\n",
    "    \n",
    "    return tokens_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets start by exploring positive sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets with positive sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    2970\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing positive tweets into new df\n",
    "df_positive = df_1[df_1['Emotion']=='Positive']\n",
    "#verifying that neutral and negative tweets have been removed\n",
    "df_positive['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW\",\n",
       " '@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.',\n",
       " \"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\",\n",
       " '#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan',\n",
       " 'Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB',\n",
       " 'Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear',\n",
       " 'Excited to meet the @samsungmobileus at #sxsw so I can show them my Sprint Galaxy S still running Android 2.1.   #fail']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing positive tweets into a list\n",
    "corpus_pos = df_positive['Text'].to_list()\n",
    "corpus_pos[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function `tokenize_lemmatize_remove_stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos_cleaned = tokenize_lemmatize_remove_stopwords(corpus_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to display most frequent 50 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "def find_frequent(tokens, n=50):\n",
    "    \"\"\"Function returns the n most common words along with their frequencies \n",
    "    based on a tokens list passed in.\n",
    "    -------------------------------\n",
    "    Arguments:\n",
    "    tokens: a tokens list\n",
    "    n: number of top words to be returned\"\"\"\n",
    "    \n",
    "    freq = FreqDist(tokens)\n",
    "    display(freq.most_common(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#sxsw', 2983),\n",
       " ('link', 1218),\n",
       " ('ipad', 1010),\n",
       " ('rt', 931),\n",
       " ('apple', 711),\n",
       " ('google', 602),\n",
       " ('2', 595),\n",
       " ('store', 554),\n",
       " ('iphone', 466),\n",
       " ('', 443),\n",
       " ('app', 387),\n",
       " ('new', 358),\n",
       " ('austin', 250),\n",
       " ('get', 181),\n",
       " ('#apple', 174),\n",
       " ('launch', 173),\n",
       " ('android', 161),\n",
       " ('party', 151),\n",
       " ('pop-up', 151),\n",
       " ('sxsw', 144),\n",
       " ('line', 143),\n",
       " ('time', 136),\n",
       " ('great', 135),\n",
       " ('via', 132),\n",
       " ('#ipad2', 129),\n",
       " ('day', 124),\n",
       " ('social', 122),\n",
       " ('free', 120),\n",
       " ('cool', 119),\n",
       " (\"i'm\", 115),\n",
       " ('like', 115),\n",
       " ('map', 115),\n",
       " ('one', 114),\n",
       " ('win', 112),\n",
       " ('today', 111),\n",
       " ('circle', 107),\n",
       " ('w', 104),\n",
       " ('go', 104),\n",
       " ('come', 103),\n",
       " ('#sxswi', 96),\n",
       " ('awesome', 93),\n",
       " ('#ipad', 93),\n",
       " ('love', 93),\n",
       " ('good', 92),\n",
       " ('network', 91),\n",
       " ('mobile', 90),\n",
       " ('temporary', 89),\n",
       " ('downtown', 88),\n",
       " ('opening', 88),\n",
       " ('people', 82)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displaying 50 most frequent words\n",
    "find_frequent(tokens_pos_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of 50 common words still contains some words that aren't useful for sentiment analysis, like '#sxsw', which can be removed. Words such as 'link' and 'rt' often refer to external links or retweets, though 'link' might have a different meaning. A function can help by selecting random tweets, checking for a specific word, and displaying the relevant tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that will help identify context for any given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_finder(word, corpus, n_samples=5, n_count=5):\n",
    "    \"\"\"\n",
    "    Displays tweets containing a specific word from random samples in the corpus.\n",
    "    \n",
    "    Args:\n",
    "    - word: the target word to search for.\n",
    "    - corpus: list of tweets or text.\n",
    "    - n_samples: number of random samples to check.\n",
    "    - n_count: number of tweets per sample.\n",
    "    \"\"\"\n",
    "    matched_tweets = []\n",
    "    \n",
    "    # Collect matching tweets from random samples\n",
    "    for _ in range(n_samples):\n",
    "        sample = np.random.choice(corpus, n_count)\n",
    "        matched_tweets.extend([tweet for tweet in sample if word in tweet])\n",
    "    \n",
    "    # Display results\n",
    "    for tweet in matched_tweets:\n",
    "        print(tweet)\n",
    "        \n",
    "    print('-----------------------------------')\n",
    "    print(f'Out of {n_samples * n_count} tweets analyzed, {len(matched_tweets)} contained the word \"{word}\".')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#SXSW crowd in Austin swarms for iPad 2 launch {link} via @mention - sadly I wasn't one of them :-( (via @mention\n",
      "RT @mention Last day to get the #LP Austin iPhone app for free {link} #travel #sxsw\n",
      "Apple opening a store in downtown ATX for this wknd ONLY.. same time as #SXSW so ppl can buy iPad 2. ÛÏ@mention {link}\n",
      "Google celebrating Pi Day in style at #SXSW -  {link}\n",
      "RT @mention Perfect #SXSW iPhone app home screen organization: {link} (I like it)\n",
      "RT @mention Yeah! Love the Google! Cheese w/ @mention @mention Maggie Mae's [pic] {link} #SXSW\n",
      "RT @mention Word to your motherboard. RT @mention You'll want these iPhone apps if you're heading to #SXSW today. 10 hot ones. {link}\n",
      "Hey, Apple fans! Get a peek at the space that's slated to be a pop-up #SXSW Apple Store tomorrow: {link}\n",
      "Apple, Google, Intel &amp; Others Get Going Gold for the Go Game  {link} #TheGoGame #SXSW\n",
      "Before It Even Begins, Apple WinsåÊ#SxSW, {link} -&gt; gonna get sum! via @mention\n",
      "Great idea behind #circles RT @mention &amp; @mention Google Launching Secret New Social Network Called Circles {link} #sxsw\n",
      "So geeky! Love it! RT @mention Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}\n",
      "-----------------------------------\n",
      "Out of 25 tweets analyzed, 12 contained the word \"link\".\n"
     ]
    }
   ],
   "source": [
    "#verifying that 'link' is used in reference to external web links\n",
    "context_finder('link', corpus_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation is that 'link' is used to refer to weblinks that were removed when data was being passed to dataset. We will thus add it to the stop list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a deeper dive on 'RT' to provide its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @mention Apple setting up temp store at #SXSW to sell the #iPad2. Crazy? Brilliant? Convenient? {link}\n",
      "brilliant. RT @mention Apple set to open popup shop in core of SXSW action  - they're going to sell iPad2 at #SXSW! {link}\n",
      "What I journal with on my iPhone: RT @mention ** Momento is on SALE for #SXSW ** - Grab it now for just 59p / 99å¢! {link}\n",
      "RT @mention &quot;I'm just learning all of these things from the screen.&quot; A 4-year old's explanation of how she instantly knew how to use an iPad. #sxsw\n",
      "-----------------------------------\n",
      "Out of 25 tweets analyzed, 4 contained the word \"RT\".\n"
     ]
    }
   ],
   "source": [
    "context_finder('RT', corpus_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that in all cases 'RT' or 'rt' is used to denote a retweet, therefore we can also add it to the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all additional words from findings to stop list\n",
    "\n",
    "extra_words = ['#sxsw', '#sxswi', 'sxsw','link','rt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the corpus with additional words (extra words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos_cleaned = tokenize_lemmatize_remove_stopwords(corpus_pos,additional_stopwords=extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 1010),\n",
       " ('apple', 711),\n",
       " ('google', 602),\n",
       " ('2', 595),\n",
       " ('store', 554),\n",
       " ('iphone', 466),\n",
       " ('', 443),\n",
       " ('app', 387),\n",
       " ('new', 358),\n",
       " ('austin', 250),\n",
       " ('get', 181),\n",
       " ('#apple', 174),\n",
       " ('launch', 173),\n",
       " ('android', 161),\n",
       " ('party', 151),\n",
       " ('pop-up', 151),\n",
       " ('line', 143),\n",
       " ('time', 136),\n",
       " ('great', 135),\n",
       " ('via', 132),\n",
       " ('#ipad2', 129),\n",
       " ('day', 124),\n",
       " ('social', 122),\n",
       " ('free', 120),\n",
       " ('cool', 119),\n",
       " (\"i'm\", 115),\n",
       " ('like', 115),\n",
       " ('map', 115),\n",
       " ('one', 114),\n",
       " ('win', 112),\n",
       " ('today', 111),\n",
       " ('circle', 107),\n",
       " ('w', 104),\n",
       " ('go', 104),\n",
       " ('come', 103),\n",
       " ('awesome', 93),\n",
       " ('#ipad', 93),\n",
       " ('love', 93),\n",
       " ('good', 92),\n",
       " ('network', 91),\n",
       " ('mobile', 90),\n",
       " ('temporary', 89),\n",
       " ('downtown', 88),\n",
       " ('opening', 88),\n",
       " ('people', 82),\n",
       " ('open', 82),\n",
       " ('#iphone', 82),\n",
       " ('got', 81),\n",
       " ('apps', 78),\n",
       " ('check', 77)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displaying 50 most frequent words\n",
    "find_frequent(tokens_pos_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the most common words include 'apple' and 'google', but there are additional words such as 'circle', 'launch' and 'store' in here that show us what the people were excited about. However, we are only looking at 50 words and there may be additional information here that we are missing. To address this, we can visualize this information in word clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets with Negative Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    569\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing positive tweets into new df\n",
    "df_negative = df_1[df_1['Emotion']=='Negative']\n",
    "#verifying that neutral and negative tweets have been removed\n",
    "df_negative['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.',\n",
       " \"@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw\",\n",
       " 'I just noticed DST is coming this weekend. How many iPhone users will be an hour late at SXSW come Sunday morning? #SXSW #iPhone',\n",
       " '@mention  - False Alarm: Google Circles Not Coming Now\\x89ÛÒand Probably Not Ever? - {link} #Google #Circles #Social #SXSW',\n",
       " 'Again? RT @mention Line at the Apple store is insane.. #sxsw']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsing tweets into a list\n",
    "corpus_neg = df_negative['Text'].to_list()\n",
    "corpus_neg[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function `tokenize_lemmatize_remove_stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_neg_cleaned = tokenize_lemmatize_remove_stopwords(corpus_neg,additional_stopwords=extra_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 179),\n",
       " ('iphone', 145),\n",
       " ('google', 136),\n",
       " ('apple', 100),\n",
       " ('2', 81),\n",
       " ('', 69),\n",
       " ('app', 60),\n",
       " ('store', 47),\n",
       " ('new', 43),\n",
       " ('like', 43),\n",
       " ('need', 35),\n",
       " ('circle', 29),\n",
       " ('design', 29),\n",
       " ('people', 29),\n",
       " ('social', 28),\n",
       " ('apps', 26),\n",
       " ('get', 25),\n",
       " ('austin', 23),\n",
       " ('think', 23),\n",
       " ('time', 23),\n",
       " ('launch', 22),\n",
       " ('one', 22),\n",
       " ('day', 21),\n",
       " ('today', 21),\n",
       " ('look', 21),\n",
       " ('line', 20),\n",
       " ('say', 20),\n",
       " ('android', 19),\n",
       " ('#ipad', 19),\n",
       " ('would', 19),\n",
       " ('network', 18),\n",
       " ('phone', 18),\n",
       " ('headache', 17),\n",
       " ('news', 17),\n",
       " ('go', 17),\n",
       " ('long', 17),\n",
       " ('product', 17),\n",
       " (\"i've\", 16),\n",
       " (\"i'm\", 16),\n",
       " ('battery', 16),\n",
       " ('user', 15),\n",
       " ('thing', 15),\n",
       " ('#apple', 15),\n",
       " ('good', 15),\n",
       " ('see', 15),\n",
       " ('much', 15),\n",
       " ('company', 15),\n",
       " ('america', 15),\n",
       " ('money', 14),\n",
       " ('major', 14)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#displaying 50 most frequent words\n",
    "find_frequent(tokens_neg_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see that Apple and Google are mentioned quite a few times in the negative tweets along with words such as 'design' and 'launch'. We can visualize and try to dig deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Positive Vs Negative Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library to visualize sentiments\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for wordcloud generation\n",
    "def generate_wordcloud(tokens, collocations=False, background_color='black', \n",
    "                       colormap = 'Greens', display=True):\n",
    "    \n",
    "    \"\"\"Function generates and returns a wordcloud based on a tokens list passed in.\n",
    "    -------------------------------\n",
    "    Arguments:\n",
    "    tokens: a tokens list\n",
    "    collocations: Whether to include collocations (bigrams) of two words\n",
    "    background_color: background color of the resulting word cloud\n",
    "    colormap: the color map for the words that will be in the word cloud\n",
    "    display: Whether to show the resulting wordcloud\"\"\"\n",
    "    \n",
    "    ## Initalize a WordCloud\n",
    "    wordcloud = WordCloud(collocations=collocations, \n",
    "                          background_color=background_color, \n",
    "                          colormap=colormap, \n",
    "                          width=500, height=300)\n",
    "\n",
    "    ## Generate wordcloud from tokens\n",
    "    wordcloud.generate(','.join(tokens))\n",
    "\n",
    "    ## Plot with matplotlib\n",
    "    if display:\n",
    "        plt.figure(figsize = (12, 15), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis('off');\n",
    "    \n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransposedFont' object has no attribute 'getbbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-263-c52260def03d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#generating word cloud for negative tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cloud_neg_w_company = generate_wordcloud(tokens_neg_cleaned, colormap='Reds', \n\u001b[0m\u001b[0;32m      3\u001b[0m                                          collocations=True)\n",
      "\u001b[1;32m<ipython-input-262-56f6daf9fe0c>\u001b[0m in \u001b[0;36mgenerate_wordcloud\u001b[1;34m(tokens, collocations, background_color, colormap, display)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m## Generate wordcloud from tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m## Plot with matplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \"\"\"\n\u001b[1;32m--> 642\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    509\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    510\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"A\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstroke_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m             \u001b[1;33m+\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;33m+\u001b[0m \u001b[0mspacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         )\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransposedFont' object has no attribute 'getbbox'"
     ]
    }
   ],
   "source": [
    "#generating word cloud for negative tweets\n",
    "cloud_neg_w_company = generate_wordcloud(tokens_neg_cleaned, colormap='Reds', \n",
    "                                         collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransposedFont' object has no attribute 'getbbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-1584c44bd914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#generating word cloud for positive tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcloud_pos_w_brand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_pos_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greens'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#generating word cloud for negative tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m cloud_neg_w_company = generate_wordcloud(tokens_neg_cleaned, colormap='Reds', \n\u001b[0;32m      5\u001b[0m                                          collocations=True)\n",
      "\u001b[1;32m<ipython-input-209-c087209620b8>\u001b[0m in \u001b[0;36mgenerate_wordcloud\u001b[1;34m(tokens, collocations, background_color, colormap, display)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m## Generate wordcloud from tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mwordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m## Plot with matplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \"\"\"\n\u001b[1;32m--> 642\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    622\u001b[0m         \"\"\"\n\u001b[0;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    509\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    510\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\learn-env\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"RGBA\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m         bbox = font.getbbox(\n\u001b[0m\u001b[0;32m    568\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransposedFont' object has no attribute 'getbbox'"
     ]
    }
   ],
   "source": [
    "#generating word cloud for positive tweets\n",
    "cloud_pos_w_brand = generate_wordcloud(tokens_pos_cleaned, colormap='Greens',collocations=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wordclouds(wc1, wc2):\n",
    "    \"\"\"Function plots two wordclouds side-by-side for easy comparison\n",
    "    -------------------------------\n",
    "    Arguments:\n",
    "    wc1: first wordcloud to be plotted\n",
    "    wc2: second wordcloud to be plotted\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(30,20), ncols=2)\n",
    "    ax[0].imshow(wc1)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].imshow(wc2)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
